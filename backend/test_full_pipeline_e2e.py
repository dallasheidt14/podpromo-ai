#!/usr/bin/env python3
"""
Full end-to-end test of the clip generation pipeline.
Tests the complete process from transcript to final clips with all improvements.
"""

import sys
import os
import json
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from services.secret_sauce_pkg.features import build_eos_index, discover_dynamic_length, compute_features_lite, build_hotness_curve
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_real_transcript():
    """Load the real transcript file"""
    transcript_path = "uploads/transcripts/c746f928-85c5-4541-8119-261e659291d8.json"
    
    try:
        with open(transcript_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        logger.info(f"Loaded transcript: {data.get('original_name', 'Unknown')}")
        logger.info(f"Duration: {data.get('duration', 0):.1f} seconds ({data.get('duration', 0)/60:.1f} minutes)")
        logger.info(f"Status: {data.get('status', 'Unknown')}")
        
        segments = data.get('transcript', [])
        logger.info(f"Number of segments: {len(segments)}")
        
        return segments, data
        
    except Exception as e:
        logger.error(f"Failed to load transcript: {e}")
        return None, None

def test_eos_index_generation():
    """Test EOS index generation with real data"""
    logger.info("Testing EOS index generation...")
    
    segments, transcript_data = load_real_transcript()
    if not segments:
        return False
    
    try:
        # Extract episode words
        episode_words = []
        for segment in segments:
            words = segment.get('words', [])
            episode_words.extend(words)
        
        # Build EOS index
        eos_times, word_end_times, eos_source = build_eos_index(segments, episode_words)
        
        logger.info(f"EOS Index Results:")
        logger.info(f"  EOS markers: {len(eos_times)}")
        logger.info(f"  Word boundaries: {len(word_end_times)}")
        logger.info(f"  Source: {eos_source}")
        
        if eos_times and word_end_times:
            duration_minutes = (word_end_times[-1] - word_end_times[0]) / 60.0
            density_per_min = len(eos_times) / duration_minutes
            logger.info(f"  Duration: {duration_minutes:.1f} minutes")
            logger.info(f"  Density: {density_per_min:.3f} markers per minute")
            
            return True
        else:
            logger.error("No EOS markers or word boundaries found")
            return False
            
    except Exception as e:
        logger.error(f"EOS index generation failed: {e}")
        return False

def test_feature_computation():
    """Test feature computation with real data"""
    logger.info("Testing feature computation...")
    
    segments, transcript_data = load_real_transcript()
    if not segments:
        return False
    
    try:
        # Extract words
        words = []
        for segment in segments:
            segment_words = segment.get('words', [])
            words.extend(segment_words)
        
        logger.info(f"Computing features for {len(words)} words...")
        
        # Compute features
        times, hook, arousal, payoff, info, q_or_list, emotion, loop = compute_features_lite(words, hop_s=0.5)
        
        logger.info(f"Feature Computation Results:")
        logger.info(f"  Time points: {len(times)}")
        logger.info(f"  Hook scores: {len(hook)}")
        logger.info(f"  Arousal scores: {len(arousal)}")
        logger.info(f"  Payoff scores: {len(payoff)}")
        logger.info(f"  Info scores: {len(info)}")
        logger.info(f"  Q/List scores: {len(q_or_list)}")
        logger.info(f"  Emotion scores: {len(emotion)}")
        logger.info(f"  Loop scores: {len(loop)}")
        
        # Check for reasonable values
        if len(times) > 0 and len(hook) > 0:
            logger.info(f"  Sample hook scores: {hook[:5]}")
            logger.info(f"  Sample arousal scores: {arousal[:5]}")
            return True
        else:
            logger.error("Feature computation failed")
            return False
            
    except Exception as e:
        logger.error(f"Feature computation failed: {e}")
        return False

def test_hotness_curve_generation():
    """Test hotness curve generation"""
    logger.info("Testing hotness curve generation...")
    
    segments, transcript_data = load_real_transcript()
    if not segments:
        return False
    
    try:
        # Extract words
        words = []
        for segment in segments:
            segment_words = segment.get('words', [])
            words.extend(segment_words)
        
        duration_s = transcript_data.get('duration', 0)
        
        # Compute features
        times, hook, arousal, payoff, info, q_or_list, emotion, loop = compute_features_lite(words, hop_s=0.5)
        
        # Test adaptive smoothing
        smooth_window = 3.0 if duration_s > 600 else 4.5
        hotness = build_hotness_curve(times, hook, arousal, payoff, info, q_or_list, emotion, loop, smooth_window=smooth_window)
        
        logger.info(f"Hotness Curve Results:")
        logger.info(f"  Duration: {duration_s:.1f}s ({duration_s/60:.1f} minutes)")
        logger.info(f"  Smooth window: {smooth_window}s")
        logger.info(f"  Hotness points: {len(hotness)}")
        logger.info(f"  Sample hotness values: {hotness[:5]}")
        
        if len(hotness) > 0:
            max_hotness = max(hotness)
            min_hotness = min(hotness)
            logger.info(f"  Hotness range: {min_hotness:.3f} to {max_hotness:.3f}")
            return True
        else:
            logger.error("No hotness values generated")
            return False
            
    except Exception as e:
        logger.error(f"Hotness curve generation failed: {e}")
        return False

def test_dynamic_discovery():
    """Test dynamic discovery with real data"""
    logger.info("Testing dynamic discovery...")
    
    segments, transcript_data = load_real_transcript()
    if not segments:
        return False
    
    try:
        # Extract words and EOS times
        words = []
        eos_times = []
        for segment in segments:
            segment_words = segment.get('words', [])
            words.extend(segment_words)
            eos_times.append(segment.get('end', 0))
        
        duration_s = transcript_data.get('duration', 0)
        
        # Create a realistic scoring function
        def realistic_score_fn(start, end):
            duration = end - start
            # Score based on duration with some content quality simulation
            if duration < 5.0:
                return 0.0
            elif duration < 15.0:
                return 0.3 + (duration - 5.0) / 10.0 * 0.2  # 0.3-0.5
            elif duration < 45.0:
                return 0.5 + (duration - 15.0) / 30.0 * 0.3  # 0.5-0.8
            else:
                return min(0.8, 0.8 - (duration - 45.0) / 60.0 * 0.2)  # 0.8-0.6
        
        logger.info(f"Dynamic Discovery Results:")
        logger.info(f"  Duration: {duration_s:.1f}s ({duration_s/60:.1f} minutes)")
        logger.info(f"  Words: {len(words)}")
        logger.info(f"  EOS markers: {len(eos_times)}")
        
        candidates = discover_dynamic_length(words, eos_times, realistic_score_fn, duration_s)
        
        logger.info(f"  Candidates found: {len(candidates)}")
        
        if candidates:
            logger.info("  Sample candidates:")
            for i, candidate in enumerate(candidates[:5]):
                duration = candidate['end'] - candidate['start']
                logger.info(f"    {i+1}: {candidate['start']:.1f}s-{candidate['end']:.1f}s ({duration:.1f}s) score={candidate['score']:.3f}")
            
            # Analyze candidate quality
            valid_candidates = 0
            total_duration = 0
            for candidate in candidates:
                duration = candidate['end'] - candidate['start']
                if 5.0 <= duration <= 120.0 and candidate['score'] > 0.1:
                    valid_candidates += 1
                    total_duration += duration
            
            logger.info(f"  Valid candidates: {valid_candidates}/{len(candidates)}")
            logger.info(f"  Total candidate duration: {total_duration:.1f}s ({total_duration/60:.1f} minutes)")
            
            return valid_candidates > 0
        else:
            logger.error("No candidates found")
            return False
            
    except Exception as e:
        logger.error(f"Dynamic discovery failed: {e}")
        return False

def test_full_pipeline():
    """Test the complete pipeline end-to-end"""
    logger.info("Testing complete pipeline end-to-end...")
    
    segments, transcript_data = load_real_transcript()
    if not segments:
        return False
    
    try:
        # Extract words and EOS times
        words = []
        eos_times = []
        for segment in segments:
            segment_words = segment.get('words', [])
            words.extend(segment_words)
            eos_times.append(segment.get('end', 0))
        
        duration_s = transcript_data.get('duration', 0)
        
        logger.info(f"Full Pipeline Test:")
        logger.info(f"  Transcript: {transcript_data.get('original_name', 'Unknown')}")
        logger.info(f"  Duration: {duration_s:.1f}s ({duration_s/60:.1f} minutes)")
        logger.info(f"  Segments: {len(segments)}")
        logger.info(f"  Words: {len(words)}")
        logger.info(f"  EOS markers: {len(eos_times)}")
        
        # Step 1: Build EOS index
        logger.info("\nStep 1: Building EOS index...")
        eos_times_unified, word_end_times, eos_source = build_eos_index(segments, words)
        logger.info(f"  EOS markers: {len(eos_times_unified)}")
        logger.info(f"  Source: {eos_source}")
        
        # Step 2: Compute features
        logger.info("\nStep 2: Computing features...")
        times, hook, arousal, payoff, info, q_or_list, emotion, loop = compute_features_lite(words, hop_s=0.5)
        logger.info(f"  Feature points: {len(times)}")
        
        # Step 3: Build hotness curve
        logger.info("\nStep 3: Building hotness curve...")
        smooth_window = 3.0 if duration_s > 600 else 4.5
        hotness = build_hotness_curve(times, hook, arousal, payoff, info, q_or_list, emotion, loop, smooth_window=smooth_window)
        logger.info(f"  Hotness points: {len(hotness)}")
        logger.info(f"  Smooth window: {smooth_window}s")
        
        # Step 4: Dynamic discovery
        logger.info("\nStep 4: Dynamic discovery...")
        def pipeline_score_fn(start, end):
            duration = end - start
            if duration < 5.0:
                return 0.0
            return min(0.8, duration / 30.0)
        
        candidates = discover_dynamic_length(words, eos_times_unified, pipeline_score_fn, duration_s)
        logger.info(f"  Candidates found: {len(candidates)}")
        
        # Step 5: Analyze results
        logger.info("\nStep 5: Analyzing results...")
        if candidates:
            valid_candidates = 0
            total_duration = 0
            score_sum = 0
            
            for candidate in candidates:
                duration = candidate['end'] - candidate['start']
                if 5.0 <= duration <= 120.0 and candidate['score'] > 0.1:
                    valid_candidates += 1
                    total_duration += duration
                    score_sum += candidate['score']
            
            avg_score = score_sum / valid_candidates if valid_candidates > 0 else 0
            coverage = (total_duration / duration_s) * 100 if duration_s > 0 else 0
            
            logger.info(f"  Valid candidates: {valid_candidates}/{len(candidates)}")
            logger.info(f"  Average score: {avg_score:.3f}")
            logger.info(f"  Total coverage: {total_duration:.1f}s ({coverage:.1f}% of episode)")
            logger.info(f"  Average candidate duration: {total_duration/valid_candidates:.1f}s" if valid_candidates > 0 else "  No valid candidates")
            
            # Show top candidates
            logger.info("\n  Top 5 candidates:")
            sorted_candidates = sorted(candidates, key=lambda x: x['score'], reverse=True)[:5]
            for i, candidate in enumerate(sorted_candidates):
                duration = candidate['end'] - candidate['start']
                logger.info(f"    {i+1}: {candidate['start']:.1f}s-{candidate['end']:.1f}s ({duration:.1f}s) score={candidate['score']:.3f}")
            
            return valid_candidates > 0
        else:
            logger.error("No candidates generated")
            return False
            
    except Exception as e:
        logger.error(f"Full pipeline test failed: {e}")
        return False

if __name__ == "__main__":
    logger.info("Starting full end-to-end pipeline test...")
    
    tests = [
        ("EOS Index Generation", test_eos_index_generation),
        ("Feature Computation", test_feature_computation),
        ("Hotness Curve Generation", test_hotness_curve_generation),
        ("Dynamic Discovery", test_dynamic_discovery),
        ("Full Pipeline", test_full_pipeline),
    ]
    
    results = []
    for test_name, test_func in tests:
        logger.info(f"\n{'='*70}")
        logger.info(f"Running: {test_name}")
        logger.info(f"{'='*70}")
        
        try:
            success = test_func()
            results.append((test_name, success))
            logger.info(f"Result: {'PASS' if success else 'FAIL'}")
        except Exception as e:
            logger.error(f"Test failed with exception: {e}")
            results.append((test_name, False))
    
    logger.info(f"\n{'='*70}")
    logger.info("FULL PIPELINE TEST SUMMARY")
    logger.info(f"{'='*70}")
    
    for test_name, success in results:
        status = "PASS" if success else "FAIL"
        logger.info(f"{test_name}: {status}")
    
    all_passed = all(success for _, success in results)
    logger.info(f"\nOverall: {'ALL TESTS PASSED' if all_passed else 'SOME TESTS FAILED'}")
    
    if all_passed:
        logger.info("\nüéâ FULL END-TO-END PIPELINE TEST SUCCESSFUL!")
        logger.info("  ‚úÖ EOS density standardization working")
        logger.info("  ‚úÖ Dynamic discovery stability improvements working")
        logger.info("  ‚úÖ Adaptive smoothing and thresholds working")
        logger.info("  ‚úÖ Retry logic preventing failures")
        logger.info("  ‚úÖ Complete pipeline generating valid candidates")
    else:
        logger.info("\n‚ùå Some pipeline components need attention")
